{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfE8O3uO5fn9CDJ1pOpo0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/velgaks/Course-Practice/blob/main/UkraineEduDataCollector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FBlNl6pkZukv",
        "outputId": "6db2fbcd-41bc-4089-d465-adf851490cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm requests pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "RKCqC7LXdEsB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EDBODataCollector:\n",
        "    def __init__(self, max_workers=3, delay_between_requests=2):\n",
        "        self.max_workers = max_workers\n",
        "        self.delay_between_requests = delay_between_requests\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "\n",
        "    def fetch_university_data(self, university_info, retry_count=3):\n",
        "        \"\"\"Fetch data for a single university with error handling\"\"\"\n",
        "        university_id = university_info[\"university_id\"]\n",
        "        university_name = university_info[\"university_name\"]\n",
        "\n",
        "        for attempt in range(retry_count):\n",
        "            try:\n",
        "                url = f\"https://registry.edbo.gov.ua/api/university/?id={university_id}&exp=xlsx\"\n",
        "                print(f\"Fetching: {university_name}\")\n",
        "\n",
        "                response = self.session.get(url, timeout=30)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                if response.headers.get('content-type', '').startswith('application/'):\n",
        "                    xlsx_data = BytesIO(response.content)\n",
        "                    xl = pd.ExcelFile(xlsx_data)\n",
        "\n",
        "                    if \"–û—Å–≤—ñ—Ç–Ω—ñ –ø—Ä–æ–≥—Ä–∞–º–∏\" in xl.sheet_names:\n",
        "                        df = xl.parse(\"–û—Å–≤—ñ—Ç–Ω—ñ –ø—Ä–æ–≥—Ä–∞–º–∏\")\n",
        "                        df[\"university_id\"] = university_id\n",
        "                        df[\"university_name\"] = university_name\n",
        "                        df.columns = df.columns.str.strip()\n",
        "\n",
        "                        print(f\"‚úì Got {len(df)} programs from {university_name}\")\n",
        "                        return df\n",
        "                    else:\n",
        "                        print(f\"‚ö† No programs sheet found for {university_name}\")\n",
        "                        return None\n",
        "                else:\n",
        "                    print(f\"‚úó Invalid response for {university_name}\")\n",
        "                    return None\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚úó Error for {university_name}, attempt {attempt + 1}: {str(e)}\")\n",
        "                if attempt < retry_count - 1:\n",
        "                    time.sleep(self.delay_between_requests * (attempt + 1))\n",
        "                else:\n",
        "                    return None\n",
        "        return None"
      ],
      "metadata": {
        "id": "3PaXxI_8dSKT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_all_data(self, uni_list_response, use_simple_method=True):\n",
        "    \"\"\"Collect data from all universities\"\"\"\n",
        "    # Filter state universities\n",
        "    state_universities = [\n",
        "        uni for uni in uni_list_response.json()\n",
        "        if uni.get(\"university_financing_type_name\") == \"–î–µ—Ä–∂–∞–≤–Ω–∞\"\n",
        "    ]\n",
        "\n",
        "    print(f\"Found {len(state_universities)} state universities\")\n",
        "\n",
        "    all_dataframes = []\n",
        "    failed_universities = []\n",
        "\n",
        "    if use_simple_method:\n",
        "        # Simple sequential processing\n",
        "        for i, uni in enumerate(state_universities):\n",
        "            print(f\"\\n[{i+1}/{len(state_universities)}] Processing: {uni['university_name']}\")\n",
        "\n",
        "            result = self.fetch_university_data(uni)\n",
        "            if result is not None:\n",
        "                all_dataframes.append(result)\n",
        "            else:\n",
        "                failed_universities.append(uni[\"university_name\"])\n",
        "\n",
        "            # Wait between requests\n",
        "            time.sleep(self.delay_between_requests)\n",
        "\n",
        "    # Combine results\n",
        "    if all_dataframes:\n",
        "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "        print(f\"\\nüéâ Success! Collected {len(combined_df)} programs from {len(all_dataframes)} universities\")\n",
        "    else:\n",
        "        combined_df = pd.DataFrame()\n",
        "        print(\"\\nüòû No data collected\")\n",
        "\n",
        "    if failed_universities:\n",
        "        print(f\"\\n‚ö† Failed universities ({len(failed_universities)}):\")\n",
        "        for name in failed_universities[:5]:  # Show first 5\n",
        "            print(f\"  - {name}\")\n",
        "        if len(failed_universities) > 5:\n",
        "            print(f\"  ... and {len(failed_universities) - 5} more\")\n",
        "\n",
        "    return combined_df, failed_universities\n",
        "\n",
        "# Add method to class\n",
        "EDBODataCollector.collect_all_data = collect_all_data"
      ],
      "metadata": {
        "id": "j41L0UHKdUUs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 - FIXED VERSION: Get University List from Excel\n",
        "print(\"Getting university list from Excel endpoint...\")\n",
        "\n",
        "try:\n",
        "    # The working endpoint returns Excel data\n",
        "    excel_url = \"https://registry.edbo.gov.ua/api/universities/\"\n",
        "    response = requests.get(excel_url, timeout=30)\n",
        "\n",
        "    print(f\"Status code: {response.status_code}\")\n",
        "    print(f\"Content type: {response.headers.get('content-type')}\")\n",
        "    print(f\"Response length: {len(response.content):,} bytes\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Parse the Excel file\n",
        "        xlsx_data = BytesIO(response.content)\n",
        "        xl = pd.ExcelFile(xlsx_data)\n",
        "\n",
        "        print(f\"‚úì Excel file loaded successfully!\")\n",
        "        print(f\"Available sheets: {xl.sheet_names}\")\n",
        "\n",
        "        # Usually the main data is in the first sheet or a sheet with obvious name\n",
        "        # Let's check each sheet\n",
        "        university_data = None\n",
        "\n",
        "        for sheet_name in xl.sheet_names:\n",
        "            try:\n",
        "                df = xl.parse(sheet_name)\n",
        "                print(f\"\\nSheet '{sheet_name}': {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "                if len(df) > 0:\n",
        "                    print(f\"  Columns: {list(df.columns)[:5]}...\")  # Show first 5 columns\n",
        "\n",
        "                    # Look for university-like data\n",
        "                    potential_id_cols = [col for col in df.columns if 'id' in col.lower() or '–∫–æ–¥' in col.lower()]\n",
        "                    potential_name_cols = [col for col in df.columns if 'name' in col.lower() or '–Ω–∞–∑–≤' in col.lower() or '–Ω–∞–π–º–µ–Ω—É–≤–∞–Ω–Ω—è' in col.lower()]\n",
        "                    potential_type_cols = [col for col in df.columns if '—Ç–∏–ø' in col.lower() or 'type' in col.lower() or '—Ñ—ñ–Ω–∞–Ω—Å—É–≤–∞–Ω–Ω—è' in col.lower()]\n",
        "\n",
        "                    print(f\"  Potential ID columns: {potential_id_cols}\")\n",
        "                    print(f\"  Potential name columns: {potential_name_cols}\")\n",
        "                    print(f\"  Potential type columns: {potential_type_cols}\")\n",
        "\n",
        "                    # If this looks like the main university list, save it\n",
        "                    if len(df) > 100 and potential_name_cols:  # Likely the main list\n",
        "                        university_data = df\n",
        "                        main_sheet = sheet_name\n",
        "                        print(f\"  ‚Üí This looks like the main university list!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error reading sheet '{sheet_name}': {e}\")\n",
        "\n",
        "        if university_data is not None:\n",
        "            print(f\"\\nüéâ Found university data in sheet '{main_sheet}'!\")\n",
        "            print(f\"Total universities: {len(university_data)}\")\n",
        "\n",
        "            # Show sample data\n",
        "            print(f\"\\nSample row:\")\n",
        "            sample = university_data.iloc[0]\n",
        "            for col in university_data.columns[:8]:  # Show first 8 columns\n",
        "                print(f\"  {col}: {sample[col]}\")\n",
        "\n",
        "            # Try to find state universities\n",
        "            # Look for columns that might indicate financing type\n",
        "            state_filter_applied = False\n",
        "            for col in university_data.columns:\n",
        "                if '—Ñ—ñ–Ω–∞–Ω—Å—É–≤–∞–Ω–Ω—è' in col.lower() or 'financing' in col.lower() or '—Ç–∏–ø' in col.lower():\n",
        "                    unique_values = university_data[col].value_counts()\n",
        "                    print(f\"\\nValues in '{col}':\")\n",
        "                    print(unique_values.head(10))\n",
        "\n",
        "                    # Look for \"–î–µ—Ä–∂–∞–≤–Ω–∞\" or similar\n",
        "                    state_mask = university_data[col].astype(str).str.contains('–î–µ—Ä–∂–∞–≤–Ω', case=False, na=False)\n",
        "                    if state_mask.sum() > 0:\n",
        "                        state_universities = university_data[state_mask].copy()\n",
        "                        print(f\"‚úì Found {len(state_universities)} state universities using column '{col}'\")\n",
        "                        state_filter_applied = True\n",
        "                        break\n",
        "\n",
        "            if not state_filter_applied:\n",
        "                print(f\"‚ö† Couldn't auto-detect state universities, using all {len(university_data)} universities\")\n",
        "                state_universities = university_data.copy()\n",
        "\n",
        "            # Save the university list for later use\n",
        "            uni_list_df = state_universities\n",
        "            print(f\"\\n‚úì Ready to collect data from {len(uni_list_df)} universities\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå Couldn't find university data in any sheet\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading university list: {e}\")\n",
        "\n",
        "# Let's also create a simple function to convert our DataFrame to the format expected by our collector\n",
        "def df_to_university_list(df):\n",
        "    \"\"\"Convert DataFrame to list format expected by our collector\"\"\"\n",
        "    universities = []\n",
        "\n",
        "    # Try to find the right columns\n",
        "    id_col = None\n",
        "    name_col = None\n",
        "\n",
        "    for col in df.columns:\n",
        "        if not id_col and ('id' in col.lower() or '–∫–æ–¥' in col.lower()):\n",
        "            id_col = col\n",
        "        if not name_col and ('name' in col.lower() or '–Ω–∞–∑–≤' in col.lower() or '–Ω–∞–π–º–µ–Ω—É–≤–∞–Ω–Ω—è' in col.lower()):\n",
        "            name_col = col\n",
        "\n",
        "    if id_col and name_col:\n",
        "        for _, row in df.iterrows():\n",
        "            universities.append({\n",
        "                \"university_id\": row[id_col],\n",
        "                \"university_name\": row[name_col],\n",
        "                \"university_financing_type_name\": \"–î–µ—Ä–∂–∞–≤–Ω–∞\"  # We already filtered for state unis\n",
        "            })\n",
        "        print(f\"‚úì Converted {len(universities)} universities to expected format\")\n",
        "        print(f\"Using ID column: '{id_col}', Name column: '{name_col}'\")\n",
        "    else:\n",
        "        print(f\"‚ùå Couldn't find ID column ({id_col}) or Name column ({name_col})\")\n",
        "        return []\n",
        "\n",
        "    return universities\n",
        "\n",
        "# Convert our DataFrame if we have it\n",
        "if 'uni_list_df' in locals():\n",
        "    uni_list_converted = df_to_university_list(uni_list_df)\n",
        "    print(f\"\\nSample converted university:\")\n",
        "    if uni_list_converted:\n",
        "        sample = uni_list_converted[0]\n",
        "        for key, value in sample.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(\"No university data available to convert\")"
      ],
      "metadata": {
        "id": "QlZKMD5mdWzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5.5 - FILTER AND EXPLORE THE DATA BETTER\n",
        "print(\"üîç Let's explore and filter the data better...\")\n",
        "\n",
        "# Check what categories of institutions we have\n",
        "print(\"\\n=== CATEGORIES OF INSTITUTIONS ===\")\n",
        "if '–ö–∞—Ç–µ–≥–æ—Ä—ñ—è –∑–∞–∫–ª–∞–¥—É –æ—Å–≤—ñ—Ç–∏' in uni_list_df.columns:\n",
        "    categories = uni_list_df['–ö–∞—Ç–µ–≥–æ—Ä—ñ—è –∑–∞–∫–ª–∞–¥—É –æ—Å–≤—ñ—Ç–∏'].value_counts()\n",
        "    print(categories)\n",
        "\n",
        "    # Filter for higher education institutions (universities)\n",
        "    university_keywords = ['–≤–∏—â–æ—ó –æ—Å–≤—ñ—Ç–∏', '—É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç', '—ñ–Ω—Å—Ç–∏—Ç—É—Ç', '–∞–∫–∞–¥–µ–º—ñ—è']\n",
        "    is_university = uni_list_df['–ö–∞—Ç–µ–≥–æ—Ä—ñ—è –∑–∞–∫–ª–∞–¥—É –æ—Å–≤—ñ—Ç–∏'].astype(str).str.contains('|'.join(university_keywords), case=False, na=False)\n",
        "\n",
        "    universities_only = uni_list_df[is_university].copy()\n",
        "    print(f\"\\n‚úì Filtered to {len(universities_only)} higher education institutions\")\n",
        "else:\n",
        "    # If no category column, filter by name patterns\n",
        "    print(\"No category column found, filtering by institution names...\")\n",
        "    university_keywords = ['—É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç', '—ñ–Ω—Å—Ç–∏—Ç—É—Ç', '–∞–∫–∞–¥–µ–º—ñ—è', '–∫–æ–ª–µ–¥–∂']\n",
        "    is_university = uni_list_df['–ù–∞–∑–≤–∞ –∑–∞–∫–ª–∞–¥—É –æ—Å–≤—ñ—Ç–∏'].astype(str).str.contains('|'.join(university_keywords), case=False, na=False)\n",
        "    universities_only = uni_list_df[is_university].copy()\n",
        "    print(f\"‚úì Filtered to {len(universities_only)} institutions with university-like names\")\n",
        "\n",
        "# Check for state/private distinction\n",
        "print(f\"\\n=== LOOKING FOR STATE VS PRIVATE DISTINCTION ===\")\n",
        "potential_ownership_columns = [col for col in uni_list_df.columns if any(keyword in col.lower() for keyword in ['–≤–ª–∞—Å–Ω', '—Ñ–æ—Ä–º–∞', '—Ç–∏–ø', '—Å—Ç–∞—Ç—É—Å', '—Ñ—ñ–Ω–∞–Ω—Å'])]\n",
        "print(f\"Potential ownership columns: {potential_ownership_columns}\")\n",
        "\n",
        "state_universities = universities_only  # Default to all universities\n",
        "for col in potential_ownership_columns:\n",
        "    print(f\"\\nValues in '{col}':\")\n",
        "    values = universities_only[col].value_counts().head(10)\n",
        "    print(values)\n",
        "\n",
        "    # Look for state indicators\n",
        "    state_indicators = ['–¥–µ—Ä–∂–∞–≤–Ω', '–∫–æ–º—É–Ω–∞–ª—å–Ω', 'public', 'state']\n",
        "    for indicator in state_indicators:\n",
        "        mask = universities_only[col].astype(str).str.contains(indicator, case=False, na=False)\n",
        "        if mask.sum() > 0:\n",
        "            state_universities = universities_only[mask].copy()\n",
        "            print(f\"‚úì Found {len(state_universities)} state institutions using '{indicator}' in column '{col}'\")\n",
        "            break\n",
        "    if len(state_universities) < len(universities_only):\n",
        "        break\n",
        "\n",
        "print(f\"\\nüéØ FINAL SELECTION: {len(state_universities)} institutions to process\")\n",
        "\n",
        "# Show some examples\n",
        "print(f\"\\n=== SAMPLE INSTITUTIONS TO PROCESS ===\")\n",
        "for i in range(min(5, len(state_universities))):\n",
        "    row = state_universities.iloc[i]\n",
        "    print(f\"{i+1}. {row['–ù–∞–∑–≤–∞ –∑–∞–∫–ª–∞–¥—É –æ—Å–≤—ñ—Ç–∏']} (ID: {row['–ö–æ–¥']})\")\n",
        "\n",
        "# Convert to the format our collector expects\n",
        "def df_to_university_list_v2(df):\n",
        "    \"\"\"Convert DataFrame to list format expected by our collector\"\"\"\n",
        "    universities = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        universities.append({\n",
        "            \"university_id\": row['–ö–æ–¥'],\n",
        "            \"university_name\": row['–ù–∞–∑–≤–∞ –∑–∞–∫–ª–∞–¥—É –æ—Å–≤—ñ—Ç–∏'],\n",
        "            \"university_financing_type_name\": \"–î–µ—Ä–∂–∞–≤–Ω–∞\"\n",
        "        })\n",
        "\n",
        "    return universities\n",
        "\n",
        "# Convert our filtered data\n",
        "final_university_list = df_to_university_list_v2(state_universities)\n",
        "print(f\"\\n‚úÖ Converted {len(final_university_list)} universities to expected format\")\n",
        "\n",
        "# Create a mock response object to work with our existing collector\n",
        "class MockResponse:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def json(self):\n",
        "        return self.data\n",
        "\n",
        "uni_list = MockResponse(final_university_list)\n",
        "print(\"‚úÖ Created mock response object for collector\")\n",
        "\n",
        "print(f\"\\nüöÄ Ready to start collection with {len(final_university_list)} institutions!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CAPFo09ffUPP",
        "outputId": "13e33cbf-43f0-489d-b690-9f6e62c1306a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Let's explore and filter the data better...\n",
            "\n",
            "=== CATEGORIES OF INSTITUTIONS ===\n",
            "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è –∑–∞–∫–ª–∞–¥—É –æ—Å–≤—ñ—Ç–∏\n",
            "–ó–∞–∫–ª–∞–¥ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ—ó (–ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ-—Ç–µ—Ö–Ω—ñ—á–Ω–æ—ó) –æ—Å–≤—ñ—Ç–∏                         1147\n",
            "–ó–∞–∫–ª–∞–¥ —Ñ–∞—Ö–æ–≤–æ—ó –ø–µ—Ä–µ–¥–≤–∏—â–æ—ó –æ—Å–≤—ñ—Ç–∏                                          694\n",
            "–ó–∞–∫–ª–∞–¥ –≤–∏—â–æ—ó –æ—Å–≤—ñ—Ç–∏                                                       521\n",
            "–Ü–Ω—à–∏–π –∑–∞–∫–ª–∞–¥ –æ—Å–≤—ñ—Ç–∏, —â–æ –Ω–∞–¥–∞—î –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω—É (–ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ-—Ç–µ—Ö–Ω—ñ—á–Ω—É –æ—Å–≤—ñ—Ç—É)     318\n",
            "–ù–∞—É–∫–æ–≤—ñ —ñ–Ω—Å—Ç–∏—Ç—É—Ç–∏ (—É—Å—Ç–∞–Ω–æ–≤–∏)                                              204\n",
            "–ó–∞–∫–ª–∞–¥ –ø—ñ—Å–ª—è–¥–∏–ø–ª–æ–º–Ω–æ—ó –æ—Å–≤—ñ—Ç–∏                                               24\n",
            "–ó–∞–∫–ª–∞–¥ –∑–∞–≥–∞–ª—å–Ω–æ—ó —Å–µ—Ä–µ–¥–Ω—å–æ—ó –æ—Å–≤—ñ—Ç–∏                                           4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "‚úì Filtered to 1419 higher education institutions\n",
            "\n",
            "=== LOOKING FOR STATE VS PRIVATE DISTINCTION ===\n",
            "Potential ownership columns: ['–§–æ—Ä–º–∞ –≤–ª–∞—Å–Ω–æ—Å—Ç—ñ']\n",
            "\n",
            "Values in '–§–æ—Ä–º–∞ –≤–ª–∞—Å–Ω–æ—Å—Ç—ñ':\n",
            "–§–æ—Ä–º–∞ –≤–ª–∞—Å–Ω–æ—Å—Ç—ñ\n",
            "–î–µ—Ä–∂–∞–≤–Ω–∞        858\n",
            "–ü—Ä–∏–≤–∞—Ç–Ω–∞        359\n",
            "–ö–æ–º—É–Ω–∞–ª—å–Ω–∞      201\n",
            "–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞      1\n",
            "Name: count, dtype: int64\n",
            "‚úì Found 858 state institutions using '–¥–µ—Ä–∂–∞–≤–Ω' in column '–§–æ—Ä–º–∞ –≤–ª–∞—Å–Ω–æ—Å—Ç—ñ'\n",
            "\n",
            "üéØ FINAL SELECTION: 858 institutions to process\n",
            "\n",
            "=== SAMPLE INSTITUTIONS TO PROCESS ===\n",
            "1. –íi–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –øi–¥—Ä–æ–∑–¥i–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–≥–æ –≥–æ—Å–ø–æ–¥–∞—Ä—Å—Ç–≤–∞ –ù–∞—Üi–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ωi–≤–µ—Ä—Å–∏—Ç–µ—Ç—É —Ö–∞—Ä—á–æ–≤–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥i–π¬ª (ID: 1014)\n",
            "2. –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –î–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É¬ª (ID: 6626)\n",
            "3. –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–í—É–≥–ª–µ–¥–∞—Ä—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ú–∞—Ä—ñ—É–ø–æ–ª—å—Å—å–∫–æ–≥–æ –¥–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É\" (ID: 6502)\n",
            "4. –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–Ü–Ω—Å—Ç–∏—Ç—É—Ç —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–æ—ó –æ—Å–≤—ñ—Ç–∏ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏\" (ID: 2118)\n",
            "5. –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª –ö–∏—ó–≤—Å—å–∫–∏–π —ñ–Ω–¥—É—Å—Ç—Ä—ñ–∞–ª—å–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ (ID: 3038)\n",
            "\n",
            "‚úÖ Converted 858 universities to expected format\n",
            "‚úÖ Created mock response object for collector\n",
            "\n",
            "üöÄ Ready to start collection with 858 institutions!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 - UPDATED: Initialize Collector and Start Collection\n",
        "\n",
        "# Initialize the collector with conservative settings\n",
        "collector = EDBODataCollector(\n",
        "    max_workers=2,              # Keep low to be nice to server\n",
        "    delay_between_requests=4    # 4 seconds between requests - be respectful!\n",
        ")\n",
        "\n",
        "# Let's start with just a small test first\n",
        "TEST_MODE = True  # Set to False when you want to process all universities\n",
        "\n",
        "if TEST_MODE:\n",
        "    # Test with first 5 institutions\n",
        "    test_data = final_university_list[:5]\n",
        "    test_response = MockResponse(test_data)\n",
        "\n",
        "    print(f\"üß™ TEST MODE: Processing {len(test_data)} institutions first\")\n",
        "    for i, uni in enumerate(test_data):\n",
        "        print(f\"  {i+1}. {uni['university_name']} (ID: {uni['university_id']})\")\n",
        "\n",
        "    print(f\"\\nStarting TEST collection...\")\n",
        "    univ_data, failed_unis = collector.collect_all_data(test_response, use_simple_method=True)\n",
        "\n",
        "    if not univ_data.empty:\n",
        "        print(f\"\\nüéâ TEST SUCCESSFUL!\")\n",
        "        print(f\"Collected {len(univ_data)} programs from test institutions\")\n",
        "        print(f\"Sample data preview:\")\n",
        "        print(univ_data.head(2))\n",
        "\n",
        "        print(f\"\\n‚úÖ Test worked! Set TEST_MODE = False to process all {len(final_university_list)} institutions\")\n",
        "        print(f\"‚ö†Ô∏è  Full processing will take several hours - make sure you're ready!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Test failed - let's debug before processing all institutions\")\n",
        "\n",
        "else:\n",
        "    # Full processing mode\n",
        "    print(f\"üöÄ FULL MODE: Processing ALL {len(final_university_list)} institutions\")\n",
        "    print(f\"‚è∞ This will take 3-4+ hours - go get dinner! üçΩÔ∏è\")\n",
        "    print(f\"üí° You can interrupt anytime with Ctrl+C and restart later\")\n",
        "\n",
        "    # Confirm before starting\n",
        "    import time\n",
        "    print(f\"\\nStarting in 10 seconds... (interrupt now if you want to test first)\")\n",
        "    for i in range(10, 0, -1):\n",
        "        print(f\"‚è≥ {i}...\", end=\" \", flush=True)\n",
        "        time.sleep(1)\n",
        "    print(f\"\\nüèÅ Starting full collection!\")\n",
        "\n",
        "    univ_data, failed_unis = collector.collect_all_data(uni_list, use_simple_method=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YmTaBRLvdY2c",
        "outputId": "50703332-9593-49f2-ed3a-4ebcd831511a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ TEST MODE: Processing 5 institutions first\n",
            "  1. –íi–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –øi–¥—Ä–æ–∑–¥i–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–≥–æ –≥–æ—Å–ø–æ–¥–∞—Ä—Å—Ç–≤–∞ –ù–∞—Üi–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ωi–≤–µ—Ä—Å–∏—Ç–µ—Ç—É —Ö–∞—Ä—á–æ–≤–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥i–π¬ª (ID: 1014)\n",
            "  2. –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –î–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É¬ª (ID: 6626)\n",
            "  3. –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–í—É–≥–ª–µ–¥–∞—Ä—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ú–∞—Ä—ñ—É–ø–æ–ª—å—Å—å–∫–æ–≥–æ –¥–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É\" (ID: 6502)\n",
            "  4. –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–Ü–Ω—Å—Ç–∏—Ç—É—Ç —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–æ—ó –æ—Å–≤—ñ—Ç–∏ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏\" (ID: 2118)\n",
            "  5. –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª –ö–∏—ó–≤—Å—å–∫–∏–π —ñ–Ω–¥—É—Å—Ç—Ä—ñ–∞–ª—å–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ (ID: 3038)\n",
            "\n",
            "Starting TEST collection...\n",
            "Found 5 state universities\n",
            "\n",
            "[1/5] Processing: –íi–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –øi–¥—Ä–æ–∑–¥i–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–≥–æ –≥–æ—Å–ø–æ–¥–∞—Ä—Å—Ç–≤–∞ –ù–∞—Üi–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ωi–≤–µ—Ä—Å–∏—Ç–µ—Ç—É —Ö–∞—Ä—á–æ–≤–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥i–π¬ª\n",
            "Fetching: –íi–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –øi–¥—Ä–æ–∑–¥i–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–≥–æ –≥–æ—Å–ø–æ–¥–∞—Ä—Å—Ç–≤–∞ –ù–∞—Üi–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ωi–≤–µ—Ä—Å–∏—Ç–µ—Ç—É —Ö–∞—Ä—á–æ–≤–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥i–π¬ª\n",
            "‚úì Got 15 programs from –íi–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –øi–¥—Ä–æ–∑–¥i–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–≥–æ –≥–æ—Å–ø–æ–¥–∞—Ä—Å—Ç–≤–∞ –ù–∞—Üi–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ωi–≤–µ—Ä—Å–∏—Ç–µ—Ç—É —Ö–∞—Ä—á–æ–≤–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥i–π¬ª\n",
            "\n",
            "[2/5] Processing: –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –î–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É¬ª\n",
            "Fetching: –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –î–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É¬ª\n",
            "‚úì Got 43 programs from –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –î–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É¬ª\n",
            "\n",
            "[3/5] Processing: –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–í—É–≥–ª–µ–¥–∞—Ä—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ú–∞—Ä—ñ—É–ø–æ–ª—å—Å—å–∫–æ–≥–æ –¥–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É\"\n",
            "Fetching: –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–í—É–≥–ª–µ–¥–∞—Ä—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ú–∞—Ä—ñ—É–ø–æ–ª—å—Å—å–∫–æ–≥–æ –¥–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É\"\n",
            "‚úì Got 12 programs from –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–í—É–≥–ª–µ–¥–∞—Ä—Å—å–∫–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ú–∞—Ä—ñ—É–ø–æ–ª—å—Å—å–∫–æ–≥–æ –¥–µ—Ä–∂–∞–≤–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É\"\n",
            "\n",
            "[4/5] Processing: –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–Ü–Ω—Å—Ç–∏—Ç—É—Ç —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–æ—ó –æ—Å–≤—ñ—Ç–∏ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏\"\n",
            "Fetching: –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–Ü–Ω—Å—Ç–∏—Ç—É—Ç —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–æ—ó –æ—Å–≤—ñ—Ç–∏ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏\"\n",
            "‚úì Got 45 programs from –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª \"–Ü–Ω—Å—Ç–∏—Ç—É—Ç —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–æ—ó –æ—Å–≤—ñ—Ç–∏ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏\"\n",
            "\n",
            "[5/5] Processing: –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª –ö–∏—ó–≤—Å—å–∫–∏–π —ñ–Ω–¥—É—Å—Ç—Ä—ñ–∞–ª—å–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏\n",
            "Fetching: –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª –ö–∏—ó–≤—Å—å–∫–∏–π —ñ–Ω–¥—É—Å—Ç—Ä—ñ–∞–ª—å–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏\n",
            "‚úì Got 23 programs from –í—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª –ö–∏—ó–≤—Å—å–∫–∏–π —ñ–Ω–¥—É—Å—Ç—Ä—ñ–∞–ª—å–Ω–∏–π —Ñ–∞—Ö–æ–≤–∏–π –∫–æ–ª–µ–¥–∂ –ö–∏—ó–≤—Å—å–∫–æ–≥–æ –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞ —ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏\n",
            "\n",
            "üéâ Success! Collected 138 programs from 5 universities\n",
            "\n",
            "üéâ TEST SUCCESSFUL!\n",
            "Collected 138 programs from test institutions\n",
            "Sample data preview:\n",
            "      –û—Å–≤—ñ—Ç–Ω—ñ–π —Å—Ç—É–ø—ñ–Ω—å –ö–æ–¥ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ          –ù–∞–∑–≤–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ  \\\n",
            "0  –ú–æ–ª–æ–¥—à–∏–π —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç               071        –û–±–ª—ñ–∫ —ñ –æ–ø–æ–¥–∞—Ç–∫—É–≤–∞–Ω–Ω—è   \n",
            "1  –ú–æ–ª–æ–¥—à–∏–π —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç               142  –ï–Ω–µ—Ä–≥–µ—Ç–∏—á–Ω–µ –º–∞—à–∏–Ω–æ–±—É–¥—É–≤–∞–Ω–Ω—è   \n",
            "\n",
            "  –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è  –ö–æ–¥ –û–ü –≤ –Ñ–î–ï–ë–û                     –ù–∞–∑–≤–∞ –û–ü –¢–∏–ø –ø—Ä–æ–≥—Ä–∞–º–∏  \\\n",
            "0           NaN           14538        –û–±–ª—ñ–∫ —ñ –æ–ø–æ–¥–∞—Ç–∫—É–≤–∞–Ω–Ω—è          NaN   \n",
            "1           NaN           15566  –ï–Ω–µ—Ä–≥–µ—Ç–∏—á–Ω–µ –º–∞—à–∏–Ω–æ–±—É–¥—É–≤–∞–Ω–Ω—è          NaN   \n",
            "\n",
            "  –°–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç –ø—Ä–æ –∞–∫—Ä–µ–¥–∏—Ç–∞—Ü—ñ—é –û–ü –î–∞—Ç–∞ –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è –¥—ñ—ó  \\\n",
            "0                                               NaT   \n",
            "1                                               NaT   \n",
            "\n",
            "  –°–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç –ø—Ä–æ –∞–∫—Ä–µ–¥–∏—Ç–∞—Ü—ñ—é —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ –î–∞—Ç–∞ –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è –¥—ñ—ó  university_id  \\\n",
            "0                              –£–î 11012321          2026-07-01           1014   \n",
            "1                              –£–î 11012322          2027-07-01           1014   \n",
            "\n",
            "                                     university_name  \n",
            "0  –íi–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –øi–¥—Ä–æ–∑–¥i–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π...  \n",
            "1  –íi–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏–π –øi–¥—Ä–æ–∑–¥i–ª ¬´–ö–∏—ó–≤—Å—å–∫–∏–π...  \n",
            "\n",
            "‚úÖ Test worked! Set TEST_MODE = False to process all 858 institutions\n",
            "‚ö†Ô∏è  Full processing will take several hours - make sure you're ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BEFORE STARTING FULL COLLECTION - RUN THIS CELL FIRST\n",
        "\n",
        "print(\"üöÄ PREPARATION FOR FULL COLLECTION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check our current status\n",
        "print(f\"‚úÖ Test completed: 138 programs from 5 institutions\")\n",
        "print(f\"üéØ Ready to process: {len(final_university_list)} total institutions\")\n",
        "print(f\"‚è±Ô∏è  Estimated time: {len(final_university_list) * 4 / 60:.1f} minutes minimum\")\n",
        "print(f\"üíæ Expected data size: ~{len(final_university_list) * 30:,} programs (rough estimate)\")\n",
        "\n",
        "# Set up automatic saving during collection\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create a timestamp for this collection run\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_dir = f\"edbo_collection_{timestamp}\"\n",
        "\n",
        "print(f\"\\nüìÅ Will save data to: {save_dir}/\")\n",
        "print(f\"üìä Backup files will be created every 100 institutions\")\n",
        "\n",
        "# Enhanced collector with auto-save capability\n",
        "class EnhancedEDBOCollector(EDBODataCollector):\n",
        "    def __init__(self, *args, save_every=100, save_dir=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.save_every = save_every\n",
        "        self.save_dir = save_dir or f\"edbo_collection_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        # Create save directory\n",
        "        if not os.path.exists(self.save_dir):\n",
        "            os.makedirs(self.save_dir)\n",
        "            print(f\"üìÅ Created directory: {self.save_dir}\")\n",
        "\n",
        "    def collect_all_data(self, uni_list_response, use_simple_method=True):\n",
        "        \"\"\"Enhanced collection with auto-save\"\"\"\n",
        "        state_universities = [\n",
        "            uni for uni in uni_list_response.json()\n",
        "            if uni.get(\"university_financing_type_name\") == \"–î–µ—Ä–∂–∞–≤–Ω–∞\"\n",
        "        ]\n",
        "\n",
        "        print(f\"Found {len(state_universities)} state universities\")\n",
        "\n",
        "        all_dataframes = []\n",
        "        failed_universities = []\n",
        "\n",
        "        for i, uni in enumerate(state_universities):\n",
        "            print(f\"\\n[{i+1}/{len(state_universities)}] Processing: {uni['university_name'][:60]}...\")\n",
        "\n",
        "            result = self.fetch_university_data(uni)\n",
        "            if result is not None:\n",
        "                all_dataframes.append(result)\n",
        "                print(f\"  ‚úÖ Success: {len(result)} programs\")\n",
        "            else:\n",
        "                failed_universities.append(uni[\"university_name\"])\n",
        "                print(f\"  ‚ùå Failed\")\n",
        "\n",
        "            # Auto-save every N institutions\n",
        "            if (i + 1) % self.save_every == 0 and all_dataframes:\n",
        "                try:\n",
        "                    partial_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "                    filename = f\"{self.save_dir}/partial_backup_{i+1}_institutions.csv\"\n",
        "                    partial_df.to_csv(filename, index=False, encoding='utf-8')\n",
        "                    print(f\"  üíæ Backup saved: {len(partial_df)} programs to {filename}\")\n",
        "                except Exception as save_error:\n",
        "                    print(f\"  ‚ö†Ô∏è Backup save failed: {save_error}\")\n",
        "\n",
        "            # Progress summary every 50 institutions\n",
        "            if (i + 1) % 50 == 0:\n",
        "                total_programs = sum(len(df) for df in all_dataframes)\n",
        "                success_rate = len(all_dataframes) / (i + 1) * 100\n",
        "                print(f\"  üìä Progress: {total_programs:,} programs collected, {success_rate:.1f}% success rate\")\n",
        "\n",
        "            time.sleep(self.delay_between_requests)\n",
        "\n",
        "        # Final combining and saving\n",
        "        if all_dataframes:\n",
        "            combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "            # Save final results\n",
        "            final_filename = f\"{self.save_dir}/final_results.csv\"\n",
        "            combined_df.to_csv(final_filename, index=False, encoding='utf-8')\n",
        "\n",
        "            # Save failed institutions list\n",
        "            if failed_universities:\n",
        "                failed_filename = f\"{self.save_dir}/failed_institutions.txt\"\n",
        "                with open(failed_filename, 'w', encoding='utf-8') as f:\n",
        "                    f.write(f\"Failed institutions ({len(failed_universities)} total):\\n\")\n",
        "                    for name in failed_universities:\n",
        "                        f.write(f\"- {name}\\n\")\n",
        "\n",
        "            print(f\"\\nüéâ COLLECTION COMPLETE!\")\n",
        "            print(f\"üìä Final results: {len(combined_df):,} programs from {len(all_dataframes)} institutions\")\n",
        "            print(f\"üíæ Saved to: {final_filename}\")\n",
        "            print(f\"‚ùå Failed: {len(failed_universities)} institutions\")\n",
        "\n",
        "        else:\n",
        "            combined_df = pd.DataFrame()\n",
        "            print(f\"\\nüòû No data collected\")\n",
        "\n",
        "        return combined_df, failed_universities\n",
        "\n",
        "# Create enhanced collector\n",
        "enhanced_collector = EnhancedEDBOCollector(\n",
        "    max_workers=2,\n",
        "    delay_between_requests=4,\n",
        "    save_every=100,  # Save backup every 100 institutions\n",
        "    save_dir=save_dir\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Enhanced collector ready!\")\n",
        "print(f\"üìÅ Save directory: {save_dir}\")\n",
        "print(f\"üíæ Auto-backup every 100 institutions\")\n",
        "print(f\"‚è±Ô∏è  Delay: 4 seconds between requests\")\n",
        "\n",
        "print(f\"\\nüö® IMPORTANT REMINDERS:\")\n",
        "print(f\"1. This will take 3-4+ hours\")\n",
        "print(f\"2. Keep your computer/browser open\")\n",
        "print(f\"3. You can interrupt with Ctrl+C anytime\")\n",
        "print(f\"4. Data is auto-saved every 100 institutions\")\n",
        "print(f\"5. Don't refresh the browser during collection\")\n",
        "\n",
        "print(f\"\\nüöÄ Ready to start? Run the next cell!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "78SMiw8ygret",
        "outputId": "630522fe-2d1e-4eaf-b900-7575f2032b10"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ PREPARATION FOR FULL COLLECTION\n",
            "==================================================\n",
            "‚úÖ Test completed: 138 programs from 5 institutions\n",
            "üéØ Ready to process: 858 total institutions\n",
            "‚è±Ô∏è  Estimated time: 57.2 minutes minimum\n",
            "üíæ Expected data size: ~25,740 programs (rough estimate)\n",
            "\n",
            "üìÅ Will save data to: edbo_collection_20250826_175246/\n",
            "üìä Backup files will be created every 100 institutions\n",
            "üìÅ Created directory: edbo_collection_20250826_175246\n",
            "\n",
            "‚úÖ Enhanced collector ready!\n",
            "üìÅ Save directory: edbo_collection_20250826_175246\n",
            "üíæ Auto-backup every 100 institutions\n",
            "‚è±Ô∏è  Delay: 4 seconds between requests\n",
            "\n",
            "üö® IMPORTANT REMINDERS:\n",
            "1. This will take 3-4+ hours\n",
            "2. Keep your computer/browser open\n",
            "3. You can interrupt with Ctrl+C anytime\n",
            "4. Data is auto-saved every 100 institutions\n",
            "5. Don't refresh the browser during collection\n",
            "\n",
            "üöÄ Ready to start? Run the next cell!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL COLLECTION CELL - RUN WHEN READY\n",
        "\n",
        "print(\"üöÄ STARTING FULL COLLECTION OF UKRAINIAN UNIVERSITY DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Final confirmation\n",
        "import time\n",
        "print(f\"üìä About to process: {len(final_university_list)} institutions\")\n",
        "print(f\"‚è±Ô∏è  Estimated time: {len(final_university_list) * 4 / 60:.0f}-{len(final_university_list) * 6 / 60:.0f} minutes\")\n",
        "print(f\"üíæ Auto-save directory: {enhanced_collector.save_dir}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  LAST CHANCE TO CANCEL!\")\n",
        "print(f\"Starting in 15 seconds... Press Ctrl+C to cancel\")\n",
        "\n",
        "try:\n",
        "    for i in range(15, 0, -1):\n",
        "        print(f\"‚è≥ {i}...\", end=\" \", flush=True)\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"\\n\\nüèÅ COLLECTION STARTED!\")\n",
        "    print(f\"üìÖ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Start the full collection\n",
        "    start_time = time.time()\n",
        "    final_data, failed_institutions = enhanced_collector.collect_all_data(uni_list, use_simple_method=True)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Final summary\n",
        "    duration = end_time - start_time\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"üéä COLLECTION COMPLETED!\")\n",
        "    print(f\"‚è±Ô∏è  Duration: {duration/60:.1f} minutes ({duration/3600:.1f} hours)\")\n",
        "    print(f\"üìä Programs collected: {len(final_data):,}\")\n",
        "    print(f\"üè´ Successful institutions: {final_data['university_name'].nunique() if not final_data.empty else 0}\")\n",
        "    print(f\"‚ùå Failed institutions: {len(failed_institutions)}\")\n",
        "    print(f\"üíæ Data saved in: {enhanced_collector.save_dir}/\")\n",
        "\n",
        "    if not final_data.empty:\n",
        "        print(f\"\\nüìà QUICK STATS:\")\n",
        "        print(f\"- Average programs per institution: {len(final_data) / final_data['university_name'].nunique():.1f}\")\n",
        "        print(f\"- Most common degree: {final_data['–û—Å–≤—ñ—Ç–Ω—ñ–π —Å—Ç—É–ø—ñ–Ω—å'].mode().iloc[0] if '–û—Å–≤—ñ—Ç–Ω—ñ–π —Å—Ç—É–ø—ñ–Ω—å' in final_data.columns else 'N/A'}\")\n",
        "        print(f\"- Total specialties covered: {final_data['–ù–∞–∑–≤–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ'].nunique() if '–ù–∞–∑–≤–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ' in final_data.columns else 'N/A'}\")\n",
        "\n",
        "        # Quick preview\n",
        "        print(f\"\\nüìã SAMPLE DATA:\")\n",
        "        print(final_data[['university_name', '–ù–∞–∑–≤–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ', '–û—Å–≤—ñ—Ç–Ω—ñ–π —Å—Ç—É–ø—ñ–Ω—å']].head())\n",
        "\n",
        "    print(f\"\\n‚úÖ SUCCESS! Your Ukrainian university data is ready for analysis!\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(f\"\\n\\n‚èπÔ∏è  Collection interrupted by user\")\n",
        "    print(f\"üíæ Check {enhanced_collector.save_dir}/ for any partial data saved\")\n",
        "    print(f\"üîÑ You can resume by adjusting the university list and running again\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\n‚ùå Unexpected error: {e}\")\n",
        "    print(f\"üíæ Check {enhanced_collector.save_dir}/ for any partial data saved\")"
      ],
      "metadata": {
        "id": "aR60QOLfg-zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what we got\n",
        "if not univ_data.empty:\n",
        "    print(\"=== COLLECTION SUMMARY ===\")\n",
        "    print(f\"üìä Total programs: {len(univ_data):,}\")\n",
        "    print(f\"üè´ Universities: {univ_data['university_name'].nunique()}\")\n",
        "    print(f\"üìã Columns available: {len(univ_data.columns)}\")\n",
        "\n",
        "    print(f\"\\n=== SAMPLE DATA ===\")\n",
        "    print(univ_data.head())\n",
        "\n",
        "    print(f\"\\n=== COLUMN NAMES ===\")\n",
        "    for i, col in enumerate(univ_data.columns):\n",
        "        print(f\"{i+1:2d}. {col}\")\n",
        "else:\n",
        "    print(\"üòû No data collected\")"
      ],
      "metadata": {
        "id": "-ADydDGudate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to different formats\n",
        "if not univ_data.empty:\n",
        "    # Save as CSV\n",
        "    univ_data.to_csv('ukrainian_universities_data.csv', index=False, encoding='utf-8')\n",
        "    print(\"‚úì Saved as CSV\")\n",
        "\n",
        "    # Save as Excel\n",
        "    univ_data.to_excel('ukrainian_universities_data.xlsx', index=False)\n",
        "    print(\"‚úì Saved as Excel\")\n",
        "\n",
        "    # Quick stats\n",
        "    print(f\"\\n=== QUICK STATS ===\")\n",
        "    print(f\"Most programs: {univ_data.groupby('university_name').size().max()}\")\n",
        "    print(f\"Average programs per uni: {univ_data.groupby('university_name').size().mean():.1f}\")\n",
        "\n",
        "    # Download files to your computer\n",
        "    from google.colab import files\n",
        "    files.download('ukrainian_universities_data.csv')\n",
        "else:\n",
        "    print(\"No data to save\")"
      ],
      "metadata": {
        "id": "BcV7UdhGdcjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the data you collected\n",
        "if not univ_data.empty:\n",
        "    print(\"=== TOP UNIVERSITIES BY PROGRAM COUNT ===\")\n",
        "    top_unis = univ_data.groupby('university_name').size().sort_values(ascending=False).head(10)\n",
        "    for uni, count in top_unis.items():\n",
        "        print(f\"{count:3d} programs - {uni}\")\n",
        "\n",
        "    print(f\"\\n=== SAMPLE PROGRAM INFO ===\")\n",
        "    sample_program = univ_data.iloc[0]\n",
        "    for col in univ_data.columns[:10]:  # Show first 10 columns\n",
        "        print(f\"{col}: {sample_program[col]}\")"
      ],
      "metadata": {
        "id": "60eHColBde6-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}